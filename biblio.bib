@book{friedman2009elements,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={Springer Series in Statistics},
  year={2009},
  publisher={New York, NY: Springer-Verlag New York}
}


@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={6},
  year={2013},
  publisher={Springer}
}

@book{russell2003modern,
  title={Artificial Intelligence: A modern approach},
  author={Russell, Stuart and Norvig, Peter},
  journal={Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs},
  volume={25},
  pages={27},
  year={2003},
  publisher={Prentice-Hall}
}

@article{ovtcharov2015accelerating,
  title={Accelerating deep convolutional neural networks using specialized hardware},
  author={Ovtcharov, Kalin and Ruwase, Olatunji and Kim, Joo-Young and Fowers, Jeremy and Strauss, Karin and Chung, Eric S},
  journal={Microsoft Research Whitepaper},
  volume={2},
  number={11},
  year={2015}
}

@article{farabet2011large,
  title={Large-scale FPGA-based convolutional networks},
  author={Farabet, Cl{\'e}ment and LeCun, Yann and Kavukcuoglu, Koray and Culurciello, Eugenio and Martini, Berin and Akselrod, Polina and Talay, Selcuk},
  journal={Scaling up Machine Learning: Parallel and Distributed Approaches},
  pages={399--419},
  year={2011},
  publisher={Cambridge University Press}
}

@online{cepea,
  author = {CEPEA-Esalq/USP},
  title = {Centro de Estudos Avançados em Economia Aplicada},
  year = 2017,
  url = {http://www.cepea.esalq.usp.br/},
  urlaccessdate = {2017-08-28}
}

@online{cna,
  author = {CNA},
  title = {Confederação da Agricultura e Pecuária do Brasil},
  year = 2017,
  url = {http://www.cnabrasil.org.br/},
  urlaccessdate = {2017-08-28}
}

@online{organicos_canal_rural,
  author = {{Canal Rural}},
  title = {O que define um produto orgânico?},
  year = 2015,
  url = {http://www.canalrural.com.br/noticias/agricultura/que-define-produto-organico-56619},
  urlaccessdate = {2017-08-29}
}

@online{organicos_carta_capital,
  author = {{Carta Capital}},
  title = {Por que o mercado de orgânicos ainda não deslanchou no Brasil?},
  year = 2015,
  url = {https://www.cartacapital.com.br/economia/por-que-o-mercado-de-organicos-ainda-nao-deslanchou-no-brasil-1987.html},
  urlaccessdate = {2017-08-29}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{Zhang2017,
author = {Zhang, Jialiang and Li, Jing},
doi = {10.1145/3020078.3021698},
isbn = {9781450343541},
journal = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA '17},
mendeley-groups = {Sorting/0{\_}GoodAbstract},
pages = {25--34},
title = {{Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=3020078.3021698},
year = {2017}
}

@inproceedings{Nurvitadhi2017_0,
author={E. Nurvitadhi and D. Sheffield and Jaewoong Sim and A. Mishra and G. Venkatesh and D. Marr},
eventtitle={International Conference on Field-Programmable Technology (FPT)},
booktitle={Proceedings},
title={Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
year={2016},
pages={77-84},
keywords={application specific integrated circuits;field programmable gate arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria 10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural networks;deep neural network;hardware acceleration;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep learning;FPGA;GPU;binarized neural networks;data analytics;hardware accelerator},
doi={10.1109/FPT.2016.7929192},
publisher = {IEEE},
month={12},
location = {Xi'an, China}
}


@article{Courbariaux2015,
annote = {Used by},
archivePrefix = {arXiv},
arxivId = {1511.00363},
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
doi = {arXiv: 1412.7024},
eprint = {1511.00363},
file = {:home/vfinotti/Downloads/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf:pdf},
issn = {10495258},
mendeley-groups = {Sorting/0{\_}GoodAbstract},
pages = {3123--3131},
title = {{BinaryConnect: Training Deep Neural Networks with binary weights during propagations}},
url = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations http://arxiv.org/abs/1511.00363},
year = {2015}
}

@inproceedings{Nurvitadhi2017_1,
 author = {Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and Boudoukh, Guy},
 title = {Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?},
 booktitle = {Proceedings},
 eventtitle = {ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
 series = {FPGA '17},
 year = {2017},
 isbn = {978-1-4503-4354-1},
 location = {Monterey, California, USA},
 pages = {5--14},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3020078.3021740},
 doi = {10.1145/3020078.3021740},
 publisher = {ACM},
 keywords = {FPGA, GPU, accelerator, deep learning, intel stratix 10}
}


@online{agro_canal_rural,
  author = {{Canal Rural}},
  title = {Agronegócio é único ponto positivo para economia brasileira, dizem especialistas},
  year = 2017,
  url = {http://www.canalrural.com.br/noticias/noticias/agronegocio-unico-ponto-positivo-para-economia-brasileira-dizem-especialistas-66935},
  urlaccessdate = {2017-09-25}
}

@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {SuggestedBruno},
month = {05},
number = {7553},
pages = {436--444},
title = {Deep learning},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
publisher={Nature Research},
volume = {521},
year = {2015}
}

@online{corda_viola_agrolink,
  author = {{Agrolink}},
  title = {Corda de viola (\textit{Ipomoea grandifolia})},
  year = 2017,
  url = {https://www.agrolink.com.br/problemas/corda-de-viola_85.html},
  urlaccessdate = {2017-09-25}
}

@incollection{NIPS2012_4824,
annote = {ImageNet 2012 competition, where CNN almonst halved the error rate of the other competing approaches, what let to the rapid adoption of CNN in image recognition and computer vision fields.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}

@article{Zhang2017,
abstract = {OpenCL FPGA has recently gained great popularity with emerg-ing needs for workload acceleration such as Convolutional Neu-ral Network (CNN), which is the most popular deep learning ar-chitecture in the domain of computer vision. While OpenCL en-hances the code portability and programmability of FPGA, it comes at the expense of performance. The key challenge is to optimize the OpenCL kernels to efficiently utilize the flexible hardware re-sources in FPGA. Simply optimizing the OpenCL kernel code th-rough various compiler options turns out insufficient to achieve de-sirable performance for both compute-intensive and data-intensive workloads such as convolutional neural networks . In this paper, we first propose an analytical performance model and apply it to perform an in-depth analysis on the resource require-ment of CNN classifier kernels and available resources on modern FPGAs. We identify that the key performance bottleneck is the on-chip memory bandwidth. We propose a new kernel design to effec-tively address such bandwidth limitation and to provide an optimal balance between computation, on-chip, and off-chip memory ac-cess. As a case study, we further apply these techniques to design a CNN accelerator based on the VGG model. Finally, we evaluate the performance of our CNN accelerator using an Altera Arria 10 GX1150 board. We achieve 866 Gop/s floating point performance at 370MHz working frequency and 1.79 Top/s 16-bit fixed-point performance at 385MHz. To the best of our knowledge, our im-plementation achieves the best power efficiency and performance density compared to existing work.},
author = {Zhang, Jialiang and Li, Jing},
doi = {10.1145/3020078.3021698},
file = {:home/finotti/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Li - 2017 - Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network.pdf:pdf},
isbn = {9781450343541},
journal = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA '17},
mendeley-groups = {Sorting},
pages = {25--34},
title = {{Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=3020078.3021698},
year = {2017}
}

@article{Sun2017,
annote = {{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}
{\#} Topics

* CNN efficiency in FPGA

* Parallelism

* Pipeline

* AlexNet

* Using 16bit fixed point

* Datasets
* 1) MNIST
* 2) CalTech 101
* 3) CIFAR-10},
author = {Sun, Fan and Wang, Chao and Gong, Lei and Zhang, Yiwei and Xu, Chongchong and Lu, Yuntao and Li, Xi and Zhou, Xuehai},
doi = {10.1007/s10766-017-0522-1},
file = {:home/finotti/Dropbox/Meus{\_}Trabalhos/USP/Artigos/003{\_}UniCNN$\backslash$: A Pipelined Accelerator Towards Uniformed Computing for CNNs.pdf:pdf},
issn = {0885-7458},
journal = {International Journal of Parallel Programming},
mendeley-groups = {Sorting},
title = {{UniCNN: A Pipelined Accelerator Towards Uniformed Computing for CNNs}},
url = {http://link.springer.com/10.1007/s10766-017-0522-1},
year = {2017}
}

@article{Zhang2017,
abstract = {OpenCL FPGA has recently gained great popularity with emerg-ing needs for workload acceleration such as Convolutional Neu-ral Network (CNN), which is the most popular deep learning ar-chitecture in the domain of computer vision. While OpenCL en-hances the code portability and programmability of FPGA, it comes at the expense of performance. The key challenge is to optimize the OpenCL kernels to efficiently utilize the flexible hardware re-sources in FPGA. Simply optimizing the OpenCL kernel code th-rough various compiler options turns out insufficient to achieve de-sirable performance for both compute-intensive and data-intensive workloads such as convolutional neural networks . In this paper, we first propose an analytical performance model and apply it to perform an in-depth analysis on the resource require-ment of CNN classifier kernels and available resources on modern FPGAs. We identify that the key performance bottleneck is the on-chip memory bandwidth. We propose a new kernel design to effec-tively address such bandwidth limitation and to provide an optimal balance between computation, on-chip, and off-chip memory ac-cess. As a case study, we further apply these techniques to design a CNN accelerator based on the VGG model. Finally, we evaluate the performance of our CNN accelerator using an Altera Arria 10 GX1150 board. We achieve 866 Gop/s floating point performance at 370MHz working frequency and 1.79 Top/s 16-bit fixed-point performance at 385MHz. To the best of our knowledge, our im-plementation achieves the best power efficiency and performance density compared to existing work.},
annote = {Arria 10 performance of 1.78 Gop/s},
author = {Zhang, Jialiang and Li, Jing},
doi = {10.1145/3020078.3021698},
file = {:home/finotti/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Li - 2017 - Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network.pdf:pdf},
isbn = {9781450343541},
journal = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA '17},
mendeley-groups = {Sorting},
pages = {25--34},
title = {{Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=3020078.3021698},
year = {2017}
}

@incollection{NIPS2016_6573,
abstract = {In this work we introduce a binarized deep neural network (BDNN) model. BDNNs are trained using a novel binarized back propagation algorithm (BBP), which uses binary weights and binary neurons during the forward and backward propagation, while retaining precision of the stored weights in which gradients are accumulated. At test phase, BDNNs are fully binarized and can be implemented in hardware with low circuit complexity. The proposed binarized networks can be implemented using binary convolutions and proxy matrix multiplications with only standard binary XNOR and population count (popcount) operations. BBP is expected to reduce energy consumption by at least two orders of magnitude when compared to the hardware implementation of existing training algorithms. We obtained near state-of-the-art results with BDNNs on the permutation-invariant MNIST, CIFAR-10 and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1602.02505},
author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and Garnett, R},
eprint = {1602.02505},
file = {:home/finotti/Dropbox/Meus{\_}Trabalhos/USP/Artigos/018{\_}Binarized neural networks.pdf:pdf},
issn = {10495258},
mendeley-groups = {Unsorted},
number = {Nips},
pages = {4107--4115},
pmid = {173025},
publisher = {Curran Associates, Inc.},
title = {{Binarized Neural Networks}},
url = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf},
year = {2016}
}

@article{Venkatesh2017,
abstract = {We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6{\%} Top-1/93{\%} Top-5 on the Imagenet object classification challenge with low-precision network$\backslash$footnote{\{}github release of the source code coming soon{\}} while reducing the compute requirement by {\~{}}3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP/mm{\^{}}2 equivalent for single-precision floating-point operations ({\~{}}2 TFLOP/mm{\^{}}2 for half-precision).},
archivePrefix = {arXiv},
arxivId = {1610.00324},
author = {Venkatesh, Ganesh and Nurvitadhi, Eriko and Marr, Debbie},
doi = {10.1109/ICASSP.2017.7952679},
eprint = {1610.00324},
file = {:home/finotti/Dropbox/Meus{\_}Trabalhos/USP/Artigos/019{\_}Accelerating Deep Convolutional Networks using low-precision and sparsity.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Accelerator,Deep Neural Networks,Ternary-weight Convolutions},
mendeley-groups = {Unsorted},
pages = {2861--2865},
title = {{Accelerating Deep Convolutional Networks using low-precision and sparsity}},
year = {2017}
}

@article{Ujiie2016,
abstract = {Convolutional neural network (CNN) is becoming pop-ular because of its great ability for accurate image recog-nition. However, the computational cost is extremely high, which increases power consumption of embedded CV sys-tems. This paper proposes an efficient computing method, LazyConvPool (LCP), and its hardware architecture to re-duce power consumption of CNN-based image recognition. The LCP exploits redundancy of operations in CNN and only executes essential convolutions by an approximated prediction technique. We also propose Sign Connect, which is a low computational-cost approximated prediction with-out any multiplications. The experimental evaluation us-ing image classification dataset shows that the proposed method reduces the power consumption by 17.8{\%}–20.2{\%} and energy consumption by 11.4{\%}–14.1{\%} while retaining recognition performance.},
annote = {{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}{\#}
{\#} Topics

* Mention of other HW processors for Image recognition that use handcrafted algorithms

* Idea: Reduce CNN redundancy to reduce energy consumption.

* Datasets
* 1) MNIST
* 2) CIFAR-10},
author = {Ujiie, Takayuki and Hiromoto, Masayuki and Sato, Takashi},
doi = {10.1109/CVPRW.2016.113},
file = {:home/finotti/Dropbox/Meus{\_}Trabalhos/USP/Artigos/007{\_}Approximated Prediction Strategy for Reducing Power Consumption of Convolutional Neural Network Processor.pdf:pdf},
isbn = {978-1-5090-1437-8},
issn = {21607516},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
mendeley-groups = {Sorting},
pages = {870--876},
title = {{Approximated Prediction Strategy for Reducing Power Consumption of Convolutional Neural Network Processor}},
year = {2016}
}

@article{Courbariaux2016c,
abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
archivePrefix = {arXiv},
arxivId = {1602.02830},
author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
doi = {10.1109/CVPR.2016.90},
eprint = {1602.02830},
file = {:home/finotti/Dropbox/Meus{\_}Trabalhos/USP/Artigos/012{\_}Binarized Neural Networks$\backslash$: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1.pdf:pdf},
isbn = {9781510829008},
issn = {1664-1078},
mendeley-groups = {Unsorted},
pmid = {23554596},
title = {{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}},
url = {http://arxiv.org/abs/1602.02830},
year = {2016}
}
